{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ0XVazsDLEM"
      },
      "source": [
        "## Example Notebook for TreeLearn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTNLHg1A4hTJ"
      },
      "source": [
        "Thank you for your interest in our TreeLearn method! With this notebook and google colab, you can try out the pipeline for segmenting a forest point cloud without installing anything on your own computer! However, to actually use our method, we recommend to set up the environment on a gpu-capable device and run the segmentation pipeline there, as described in our repository.\n",
        "\n",
        "You need to be signed in with your google account. Please also make sure that you are connected to a gpu runtime by by selecting 'runtime' change runtime to e.g. T4 GPU. The following code snippet will show a table with gpu information if you are connnected to a gpu runtime. To run the code snippet, simply click on the left edge. or press (Ctrl + enter) after selecting it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI42xeR55Qsl",
        "outputId": "f020121a-7745-48a9-90f7-f536f1d5c4c6"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCl6z0uNseOV"
      },
      "source": [
        "The following two code snippets are necessary to set up the environment and download the model weights. Simply run them before continuing. It takes 2 to 3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_SYchH0wLXqx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# install environment\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
        "!pip install timm==0.6.12\n",
        "!pip install tensorboard\n",
        "!pip install tensorboardX\n",
        "!pip install laspy[lazrs]==2.5.1\n",
        "!pip install munch==2.5.0\n",
        "!pip install pandas==2.0.0\n",
        "!pip install plyfile==0.9\n",
        "!pip install pyyaml==6.0\n",
        "!pip install scikit-learn==1.2.2\n",
        "!pip install six==1.16.0\n",
        "!pip install tqdm==4.65.0\n",
        "!pip install open3d-cpu==0.17.0 --default-timeout=100\n",
        "!pip install jakteristics==0.5.1\n",
        "!pip install shapely==2.0.1\n",
        "!pip install geopandas==0.12.2\n",
        "!pip install alphashape==1.3.1\n",
        "!pip install spconv-cu114 --default-timeout=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0qxJEvNu9kGm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/ecker-lab/TreeLearn.git\n",
        "%cd TreeLearn\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "!mkdir data\n",
        "!mkdir checkpoints\n",
        "!mkdir pipeline\n",
        "!mkdir pipeline/forests\n",
        "path = \"/content/checkpoints/model_weights.pth\"\n",
        "!wget https://data.goettingen-research-online.de/api/access/datafile/:persistentId?persistentId=doi:10.25625/VPMPID/1JMEQV -O $path\n",
        "\n",
        "%cd TreeLearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra8mJVagRvlI"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INqgac0r0KQJ"
      },
      "source": [
        "We first need to decide which point cloud we want to segment. The following code snippet downloads an example point cloud segment that we did not train on.\n",
        "If you want to try out another forest point cloud, replace the download with your own. Make sure that the file is in the .npy, .npz, .las, .laz or .txt file format and the total size of the forest plot should be around 1600 m^2 at maximum. Please note that with a forest point cloud of this size the segmentation took in our runs around 15 minutes in google colab due to limited computation resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25148d-10JxJ",
        "outputId": "b9f66503-1a21-48f7-8c85-a8a5853a2e10"
      },
      "outputs": [],
      "source": [
        "forest_name = \"plot_7_cut.laz\"\n",
        "path = \"/content/pipeline/forests/\" + forest_name\n",
        "!wget https://data.goettingen-research-online.de/api/access/datafile/:persistentId?persistentId=doi:10.25625/VPMPID/0WDXL6 -O $path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZREpAa-s0Fp"
      },
      "source": [
        "\n",
        "To run the TreeLearn pipeline interactively in google colab, we import the function run_treelearn_pipeline. This function takes as argument the config dict. \n",
        "We import the pipeline.yaml as the config dict and print it. We adjust some entries in the config dict to fit to the setting in google colab and speed up the pipeline. We also initialize the logger so that the progress in the pipeline is printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "bPbQGPxNdwzL",
        "outputId": "2add54bd-b356-423d-dc06-87121e565761"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/TreeLearn/tools/pipeline\")\n",
        "from pipeline import run_treelearn_pipeline\n",
        "import argparse, pprint\n",
        "from tree_learn.util import get_config\n",
        "\n",
        "config_path = \"/content/TreeLearn/configs/pipeline/pipeline.yaml\"\n",
        "config = get_config(config_path)\n",
        "\n",
        "# adjust config\n",
        "config.forest_path = \"/content/pipeline/forests/\" + forest_name\n",
        "config.dataset_test.data_root = \"/content/pipeline/tiles\"\n",
        "config.tile_generation = True\n",
        "config.pretrain = \"/content/checkpoints/model_weights.pth\"\n",
        "config.sample_generation.stride = 0.9 # small overlap of tiles\n",
        "config.shape_cfg.outer_remove = False # default value = 13.5\n",
        "config.save_cfg.save_treewise = False\n",
        "config.save_cfg.return_type = \"voxelized_and_filtered\"\n",
        "print(pprint.pformat(config.toDict(), indent=2))\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(\"TreeLearn\")\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "logging.basicConfig()\n",
        "ch = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "ch.setFormatter(formatter)\n",
        "logger.addHandler(ch)\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6neoLf6l2zcF"
      },
      "source": [
        "After having set all the correct settings in the config file, it remains to run the pipeline. Please keep in mind that fully running it for the example point cloud takes around 12 minutes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKh7yZ3ld1o3"
      },
      "outputs": [],
      "source": [
        "# run pipeline\n",
        "run_treelearn_pipeline(config)\n",
        "# runtime ~ 12 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zroEoOMU5DOO"
      },
      "source": [
        "If everything has run as expected, the segmented point cloud is now saved in the .laz format with labels in the directory /content/pipeline/results. It is also saved in the .npz format. You can easily download it by right-clicking and selecting download. **Please note that, to speed up the pipeline for demonstration purposes, we do not remove a buffer of 13.5 meters at the edge of the input point cloud. This is usually recommended since points at the edge do not have the necessary context for making accurate predictions. The predictions at the edge of the example point cloud do not represent the capabilities of TreeLearn.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import laspy\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.9.1+cu111\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "LaszipError",
          "evalue": "reading point 0 of 85289646 total points",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLaszipError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m las_file \u001b[38;5;241m=\u001b[39m \u001b[43mlaspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/pengzhen/code/pointcloud_dataset_set/dataset/TreelearnDataset/pipeline/L1W/forest/L1W.laz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/mink_py38_cuda111/lib/python3.8/site-packages/laspy/lib.py:255\u001b[0m, in \u001b[0;36mread_las\u001b[0;34m(source, closefd, laz_backend, decompression_selection)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Entry point for reading las data in laspy\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03mReads the whole file into memory.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    The ``decompression_selection`` parameter.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_las(\n\u001b[1;32m    250\u001b[0m     source,\n\u001b[1;32m    251\u001b[0m     closefd\u001b[38;5;241m=\u001b[39mclosefd,\n\u001b[1;32m    252\u001b[0m     laz_backend\u001b[38;5;241m=\u001b[39mlaz_backend,\n\u001b[1;32m    253\u001b[0m     decompression_selection\u001b[38;5;241m=\u001b[39mdecompression_selection,\n\u001b[1;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/mink_py38_cuda111/lib/python3.8/site-packages/laspy/lasreader.py:126\u001b[0m, in \u001b[0;36mLasReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LasData:\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Reads all the points that are not read and returns a LasData object\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    This will also read EVLRS\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_points\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     las_data \u001b[38;5;241m=\u001b[39m LasData(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader, points\u001b[38;5;241m=\u001b[39mpoints)\n\u001b[1;32m    129\u001b[0m     shall_read_evlr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mminor \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mnumber_of_evlrs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevlrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     )\n",
            "File \u001b[0;32m~/.conda/envs/mink_py38_cuda111/lib/python3.8/site-packages/laspy/lasreader.py:107\u001b[0m, in \u001b[0;36mLasReader.read_points\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, points_left)\n\u001b[1;32m    106\u001b[0m r \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mPackedPointRecord\u001b[38;5;241m.\u001b[39mfrom_buffer(\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoint_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_n_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader\u001b[38;5;241m.\u001b[39mpoint_format\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m<\u001b[39m n:\n\u001b[1;32m    110\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould only read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(r)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of the requested \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m points\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.conda/envs/mink_py38_cuda111/lib/python3.8/site-packages/laspy/_compression/laszipbackend.py:78\u001b[0m, in \u001b[0;36mLaszipPointReader.read_n_points\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_n_points\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytearray\u001b[39m:\n\u001b[1;32m     77\u001b[0m     points_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_size)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munzipper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m points_data\n",
            "\u001b[0;31mLaszipError\u001b[0m: reading point 0 of 85289646 total points"
          ]
        }
      ],
      "source": [
        "las_file = laspy.read('/home/pengzhen/code/pointcloud_dataset_set/dataset/TreelearnDataset/pipeline/L1W/forest/L1W.laz')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
